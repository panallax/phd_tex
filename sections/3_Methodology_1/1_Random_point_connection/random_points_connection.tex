\documentclass[../../../main.tex]{subfiles}
\begin{document}
One of the simplest ways to generate stochastic reticular structures is to generate random points within the volume of interest and connect them according to the desired conditions.
This proposal is divided into two different problems to be solved: generating random points within a volume and connecting pairs of points in a controlled manner. 
This section aims to show the challenges that arose and how both problems were solved.

\subsection{Point-in-volume problem}
The geometric problem of determining whether a point is inside a volume (also called ‘point-in-volume test’) is a classic problem in computational geometry and has several formulations depending on the representation of the volume. 
Given an implicit function describing the volume in the form $f(x,y,z) \leq 0$, it is trivial to know whether a point $p$ is inside or outside the domain of the function. 
If the function is evaluated at such a point, the following situations can be obtained:

\begin{itemize}
    \item $f(p) < 0$, the point is inside the volume
    \item $f(p) = 0$, the point lies on the volume's surface
    \item $f(p) > 0$, the point is outside the volume
\end{itemize}

However, it is very uncommon to have the implicit function of a volume.
Typically, in computational geometry, the volume is represented as an object, comprising a collection of nodes and triangles.
The most common method used to solve this problem in that situation is called \textit{ray casting}.
This method is based on counting the number of times a ray cast from the point \textit{p} in an arbitrary direction, typically \textit{+x} or \textit{+z}, intersects the surface of the volume.
If the number of intersections is even, then the point is outside the volume. 
Otherwise, it is inside.
This method has several drawbacks, the main one being that it is very computationally expensive. 
For each point, a line must be constructed in the desired direction from that point, and the line-triangle intersection problem must be solved. 
Given $T$ triangles and $m$ points, the complexity of this method is $\mathcal{O}(mT)$. 
However, there are different ways to reduce this complexity.
Currently, ray casting algorithms use Bounding Volume Hierarchy (BVH) structures to reduce the complexity to $\mathcal{O}(TlogT+mlogT)$.
In these cases, the process is divided into two parts: first, the structure is built ($\mathcal{O}(TlogT)$) and then the relative position of the $m$ points is calculated ($\mathcal{O}(mlogT)$).

Ray casting methods are useful when the number of points, $m$, is small or it is not necessary to check all the points at a time.
But, to reduce the algorithm complexity and accelerate the processing time, it would be better to use a vectorised method that allows checking multiple points at the same time.
To do so, given the vertex of the volume, its Delaunay tetrization is obtained.
And then, it is checked whether point \textit{p} is inside any of the simplices (tetrahedra in 3D) using barycentric coordinates or equivalents.
A point \( p \) lies inside a tetrahedron with vertices \( A, B, C, D \) if there exist barycentric coordinates \( \lambda_1, \lambda_2, \lambda_3, \lambda_4 \) such that
\[
p = \lambda_1 A + \lambda_2 B + \lambda_3 C + \lambda_4 D,
\]
with the conditions
\[
\lambda_i \geq 0 \quad \text{for all } i \in \{1, 2, 3, 4\}, \quad \text{and} \quad \lambda_1 + \lambda_2 + \lambda_3 + \lambda_4 = 1.
\]
These conditions define a convex combination of the tetrahedron’s vertices. 
If all the barycentric coordinates satisfy them, the point \( P \) lies strictly inside or on the boundary of the simplex.
Given \textit{n} vertices, creating the tetrization has a complexity $\mathcal{O}(n^{2})$.
But, checking if a point is inside the volume has a complexity $\mathcal{O}(logN)$.
Therefore, the complete process has a complexity $\mathcal{O}(n^{2} + mlogN)$, which is smaller than ray casting's complexity if $m \gg n$.
This was done by using \textit{Scipy}'s library function $Delaunay(n).find\_simplex(p)
$, which returns the index of the simplex in which each point is located or $-1$ if it is not located within any simplex and is therefore outside the volume.


\subsection{Random points generation inside a volume}

Once the method for testing whether a point is inside a volume is established, the next step is to generate points within the volume.
The easiest way to achieve this is by generating random points uniformly inside the bounding box of the volume and then removing the points that are not inside the volume, as explained above.
But this is when one realises that letting randomness dominate the problem only leads to unrealistic and suboptimal solutions.
Randomness implies unpredictability and, therefore, that any result is correct. 
Obviously, this makes no sense in the macroscopic world in which we live. 
One possible outcome of the process of generating \textit{n} points randomly within the volume is that the \textit{n} points would have the same coordinates. 
Or that all the points would be clustered in the corners of the volume. 
This is not logical and, therefore, not every result is valid.

To solve this problem, the Fast Poisson-disk Sampling algorithm \cite{Bridson} is used to generate points randomly within the volume. 
This algorithm ensures that the points are uniformly and evenly distributed, ensuring that two points are not located at a minimum imposed distance. 
Therefore, it avoids point clusters and minimises gaps while ensuring local randomness. 
However, it limits the number of points generated depending on the minimum radius set.
In the \textcolor{blue}{Figure} \ref{fig:poisson}, a comparison between the uniform and Poisson-disk sampling is shown.

\begin{figure}[!htbp]
    \centering
    \\includegraphics[width =\textwidth]{imgs/poisson.svg.pdf}
    \caption{Comparison between uniform and Poisson-disk samplings of 500 points in a 1 \textit{mm} side square. A minimum distance of 0.05 \textit{mm} was used for the Poisson-disk sampling.}
    \label{fig:poisson}
\end{figure}

The difference between the two distributions is quite evident. 
While the uniform distribution has clusters of points and empty areas, the Poisson-disk distribution achieves a more evenly distributed result. 
In addition, a circle has been added to each of the points in the latter distribution, with a radius equal to the minimum radius set in the example, to show how well this algorithm works.
The only problem is that control over the final number of points is lost. 
In the example, 500 points were used, distributed in a square with a side length of 1 mm and a minimum radius of 0.05 \textit{mm}. 
The result of this is that the Poisson-disk distribution generated 247 points, while the uniform distribution generated the initial 500.
Despite this, the result obtained is much more realistic and logical.

To generate the points within the volume, the \textit{PoissonDisk} function within the Quasi-Montecarlo submodule of the \textit{Scipy} library was used. 
This function implements the Fast Poisson-disk Sampling algorithm with various additional optimisations. 
However, it only generates the sample within a 1 \textit{mm} side cube in 3D. 
Therefore, in order to fill any volume, the bounding boxes are voxelised with 1 \textit{mm} side voxels.
And in each of these voxels, the points are generated randomly. 
An example of the result of applying this logic to a 30x20 \textit{mm} cylindrical volume can be seen in \textcolor{blue}{Figure} \ref{fig:random_points}.

\begin{figure}[!htbp]
    \centering
    \\includegraphics[width =0.5\textwidth]{imgs/random_points.svg.pdf}
    \caption{Random points generated using a Poisson-disk distribution inside a 30x20 \textit{mm} cylinder. In orange, the perimeters of the top and bottom bases are plotted to represent the boundaries of the cylinder.}
    \label{fig:random_points}
\end{figure}

\subsection{Joining random points}

Once the task of generating random points inside a volume is completed, the only remaining task is to join those points together.
For two points to be joined, the connection must have an angle of at least 45$^{\circ}$ with respect to the horizontal.
This angle is calculated using the scalar product of the vector connecting both points, and the direction vector of the \textit{z}-axis as follows:
\[
\theta = \arccos\left( \frac{\vec{a} \cdot \vec{b}}{ \| \vec{a} \| \, \| \vec{b} \| } \right)
\]
Initially, every pair of points forming an angle exceeding 45$^{\circ}$ was connected. Yet, this approach led to too many connections with excessive lengths. Consequently, it was decided that the unions' behaviour also needs to be regulated.

The first change implemented to control the connections of each node was to restrict the number of possible candidate nodes.
To do this, the 30 closest neighbours were sought for each node, and only those above the analysed node were selected. 
It was decided to examine only higher nodes to avoid duplicate connections and to ensure that only nodes above or below the analysed node were joined. 
As the points are ordered according to their \textit{z}-coordinate, the structure is built from the bottom up, thus ensuring connectivity. 
The higher nodes were then filtered according to their angles, removing those that did not meet the angularity condition from the list of candidates. 
These filters reduced the number of candidate points by 80-90\%, which is why it was decided to search for so many neighbours. 
However, in some cases, the number of candidates was higher. 
Therefore, with the intention of homogenising the connections of each node, it was decided to limit the maximum number of connections, upper and lower, to a number \textit{n}. 
This made it necessary to sort the candidates according to their angle and distance from the point. 
It was decided to prioritise the closest candidates and those with an angle closest to 45$^{\circ}$.
To do this, they are assigned a score according to the following expression:
\[
node\:score = \frac{\frac{\alpha - \frac{\pi}{4}}{2\pi} + \frac{d_n}{d_{max}}}{n_{max} - n}
\]
Where $\alpha$ is the angle formed with the point, $d_n$ is the distance to the node, $d_{max}$ is the maximum distance separating a point and a neighbour among the entire set of points and neighbours, $n_{max}$ is the maximum number of connections allowed, and $n$ is the current number of connections on that candidate.
The divisor $n_{max} - n$ was included to prioritise the less connected candidates.
While the dividend was included to normalise the angle and the distance of each candidate.

This method caused many problems that could not be solved. 
\textcolor{blue}{Figure} \ref{fig:first_test_1} shows an example of a point cloud that has been joined using this method. 
This point cloud actually consists of 100 points that have been randomly generated within a 30x20 \textit{mm} cylinder. 
However, in order not to saturate the image, all points on the surface of the volume were removed. 
As can be seen, the image shows several details of the problems associated with this method. 
Each of these will be explained below:
\begin{figure}[!htbp]
    \centering
    \\includegraphics[width =1.1\textwidth]{imgs/first_test_1.svg.pdf}
    \caption{Example of graph-like structure obtained from linking each node with its closest neighbours that meet the geometrical criteria. Also, four details of different issues related to this methodology are shown. This structure has been manipulated so that isolated points appear and the issues can be analysed clearly.}
    \label{fig:first_test_1}
\end{figure}

\subsubsection{Isolated points (southern detail)}

Although the Poisson-disk sample distributes the points more evenly, they are still generated randomly, and therefore suboptimally. 
As can be seen in the detail, there may be situations in which a point has no valid candidates around it. 
For example, it may be that all the points above it do not meet the angularity condition or that, at the time of processing, all its upper neighbours already have the maximum number of connections. 
It should be noted that in the case of the image, this should not occur, but this situation has been forced in a controlled environment to facilitate compression for the reader. 
Normally, isolated points tend to appear in closer, more interior positions within the point cloud.

\subsubsection{Points without connections above (eastern detail)}

This problem is similar to the previous one, but with the difference that these points do have connections below them. 
The reasons why this happens are the same as in the previous case. 
However, the most common situation causing the loss of continuity is the saturation of the upper connections by the points processed previously. 
This was attempted to be solved by using ball trees for neighbour search, in order to avoid very long connections that could saturate future areas to be processed and cause these situations to occur. 
However, reducing the search by distance only further reduced the number of available candidates.

\subsubsection{Points with few connections above (northern detail)}

This problem is similar to the previous two, but in this case, there is continuity in the structure. 
This case could be considered a kind of false positive because it allows for continuity, but has so few connections above it that it is structurally unfeasible and should not be allowed.
For a structure to be stable, it should have at least four upper and lower connections that prevent the movement of nodes in any direction. 
The reasons why this situation occurs are the same as in the previous situations.

\subsubsection{Crossing edges (western detail)}

In this detail, you can see how the blue segment cuts through all the others that converge at the node in the centre. 
The blue segment does not belong to that node either, but arises from a lower node and connects to a point above. 
This happens because when searching for nearby neighbours, distance is not taken into account; instead, the \textit{n }closest nodes are chosen. 
Due to the unpredictability with which nodes are generated, this can lead to a node having its \textit{n} neighbours very far away, causing the connection with them to intersect with other connections. 
Obviously, this is something that should also be avoided. 
Physically, it does not make sense for two segments to be intersecting, and in addition, for the segments to be printable in a non-planar way, they must be at a minimum distance from the extruder radius. 
Therefore, crossing edges make structures unprintable in a non-planar way.

A significant amount of effort was put into fixing these errors, but due to the randomness present in the point generation process, it was not possible to guarantee that this method would always work. Therefore, it was decided to discard it.

\end{document}